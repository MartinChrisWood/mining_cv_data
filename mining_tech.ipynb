{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Location & Skills/Technologies Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import log\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a word cost dictionary based on Zipf's law, for the english language\n",
    "with open(\"./data/wordninja_words.txt\", \"r\") as f:\n",
    "    words = f.read().split()\n",
    "\n",
    "wordcost = dict((k, log((i+1)*log(len(words)))) for i,k in enumerate(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CV data\n",
    "First, bit of an encoding/formatting problem;  It's not a true json file.  Rather than just load it as text and clean it\n",
    "we fix it by reading in lines, appending the missing formatting and saving back to file (neater)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open(\"./data/Entity Recognition in Resumes.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/ER_data_cleaned.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"[\")\n",
    "    f.write(\",\\n\".join([x for x in lines]))\n",
    "    f.write(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"content\": \"Abhishek Jha\\\\nApplication Development Associate - Accenture\\\\n\\\\nBengaluru, Karnataka - E'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/ER_data_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dat = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and clean the cv text to create the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_content(text):\n",
    "    \"\"\" Specific to this data, cleaning up a lot of errant formatting\"\"\"\n",
    "    # Removes errant Indeed.com urls\n",
    "    text = \" \".join([x for x in text.split() if \".com\" not in x])\n",
    "    text = \" \".join([x for x in text.split() if \"http\" not in x])\n",
    "    \n",
    "    # Remove the section labels from Indeed.com's website\n",
    "    text = \" \".join([x for x in text.split() if not x.isupper() and len(x) > 3])\n",
    "    \n",
    "    # Reduce to only alphanumeric\n",
    "    text = re.sub(r\"[^a-zA-Z0-9 \\-+#']\", \"\", text)\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [clean_content(example['content']) for example in dat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Afreen Jamadar Active member Committee Third year Sangli Maharashtra Email Indeed wish knowledge skills conceptual understanding create excellent team environments work consistently achieving organization objectives believes taking initiative work excellence work Active member Committee Third year Cisco Networking Kanpur Uttar Pradesh organized Techkriti Kanpur Azure Skynet Quick learning ability hard working 2017 Bachelor Engg Information Technology Shivaji University Kolhapur Kolhapur Maharashtra 2016 Database Less than year Less than year Linux Less than year Less than year Less than year Programming Languages Java net php Designing Operating Systems Windows Windows Server 2003 Linux Database Access Server 2008 Oracle 10g MySql'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Countries Data\n",
    "Country names from https://datahub.io/core/country-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>AF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Åland Islands</td>\n",
       "      <td>AX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albania</td>\n",
       "      <td>AL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American Samoa</td>\n",
       "      <td>AS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name Code\n",
       "0     Afghanistan   AF\n",
       "1   Åland Islands   AX\n",
       "2         Albania   AL\n",
       "3         Algeria   DZ\n",
       "4  American Samoa   AS"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_df = pd.read_csv(\"./data/country_list.csv\")\n",
    "countries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = set(countries_df['Name'].apply(lambda x: x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>subcountry</th>\n",
       "      <th>geonameid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>les Escaldes</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>Escaldes-Engordany</td>\n",
       "      <td>3040051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>3041563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Umm al Qaywayn</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Umm al Qaywayn</td>\n",
       "      <td>290594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ras al-Khaimah</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Raʼs al Khaymah</td>\n",
       "      <td>291074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Khawr Fakkān</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Ash Shāriqah</td>\n",
       "      <td>291696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name               country          subcountry  geonameid\n",
       "0      les Escaldes               Andorra  Escaldes-Engordany    3040051\n",
       "1  Andorra la Vella               Andorra    Andorra la Vella    3041563\n",
       "2    Umm al Qaywayn  United Arab Emirates      Umm al Qaywayn     290594\n",
       "3    Ras al-Khaimah  United Arab Emirates     Raʼs al Khaymah     291074\n",
       "4      Khawr Fakkān  United Arab Emirates        Ash Shāriqah     291696"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_df = pd.read_csv(\"./data/world-cities.csv\")\n",
    "cities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_lookup = {row['name'].lower(): row['country'].lower() for index, row in cities_df.iterrows()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Properties of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_countries(tokens, countries=countries):\n",
    "    \"\"\" Expects a list of tokens that may or may not be country names. \"\"\"\n",
    "    return list(countries.intersection(set([x.lower() for x in tokens])))\n",
    "\n",
    "def find_countries_by_city(tokens, cities_lookup=cities_lookup):\n",
    "    \"\"\" Takes a list of tokens that may or may not be city names. \"\"\"\n",
    "    countries = [cities_lookup.get(token.lower(), 0) for token in tokens if not token.islower() and not token.isupper()]\n",
    "    \n",
    "    return [country for country in countries if country != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bulgaria', 'albania']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_countries([\"Albania\", \"Bulgaria\", \"Dreaming City\", \"Narnia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['united states', 'united kingdom', 'canada', 'mali', 'united states']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_countries_by_city([\"Newport\", \"London\", \"Sydney\", \"Timbuktu\", \"Washington\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(doc):\n",
    "    \"\"\"\n",
    "    Expects SpaCy parsed object. Outputs dict of lists of extracted words by\n",
    "    various categories.\n",
    "    \"\"\"\n",
    "    doc_components = {}\n",
    "    parsed = nlp(doc)\n",
    "    \n",
    "    # Proper nouns extracted, because they tend to be the names of software packages etc\n",
    "    doc_components[\"proper_nouns\"] = [token.text for token in parsed if token.pos_ == 'PROPN']\n",
    "    \n",
    "    # Common nouns extracted, because they tend to be the names of soft skills-related things\n",
    "    doc_components[\"common_nouns\"] = [token.text for token in parsed if token.pos_ == 'NOUN']\n",
    "    \n",
    "    # Dates extracted through SpaCy's Named Entity Recognition\n",
    "    doc_components[\"dates\"] = [X.text for X in parsed.ents if X.label_ == 'DATE']\n",
    "    \n",
    "    doc_components[\"countries\"] = find_countries([X.text for X in parsed])\n",
    "    \n",
    "    #if len(doc_components[\"countries\"]) == 0:\n",
    "    #    doc_components[\"countries\"] = find_countries_by_city([X.text for X in parsed])\n",
    "        \n",
    "    return doc_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_docs = []\n",
    "for doc in corpus:\n",
    "    parsed_doc = extract_keywords(doc)\n",
    "    parsed_doc['text'] = doc\n",
    "    parsed_docs.append(parsed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Arun Elumalai Tester Chennai Tamil Nadu Email Indeed Months Experience Tester Software Testing Mainframe Experience Automation Functional testing Regression Testing Involvement preparation Test scenarios Test cases executing same Defect reporting tracking Rational Quality Manager Preparation test closure reports Tester Accenture November 2016 March 2018 Associate Software Engineer Accenture Services 2016 2018 Domain Financial Services Payments Domain Application VisionPLUS Client First Data Corporation Role Tester Application VisionPLUS Description Responsibilities Have worked functional releases tested across clients region Performed system integration testing clients that came into VisionPlus Automated manual scripts Regression Testing Executing same using Selenium driver through Sauce Labs Performed Testing First Apply First Online Tested various functionalities credit card life cycle like account boarding embossing accountcard transfer replacement reissue cards Tested manual auto enrollment offers cashback offers Rising Star Award year 2017 Bachelor Engineering Automobile Engineering Venkateswara College Engineering Chennai Tamil Nadu 2012 2016 Less than year Less than year Less than year Less than year Less than year Selenium Selenium Webdriver Testing Functional Testing Automation Testing Regression Testing Quality Assurance Languages Python SoftwareTools Selenium Sauce Labs Jenkins Creo parametric 20 Catia Ansys'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_docs[7]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Khandai',\n",
       " 'Operational',\n",
       " 'Analyst',\n",
       " 'Engineer',\n",
       " 'Bengaluru',\n",
       " 'Karnataka',\n",
       " 'Email',\n",
       " 'Database',\n",
       " 'Administration',\n",
       " 'System',\n",
       " 'Analysis',\n",
       " 'Design',\n",
       " 'Development',\n",
       " 'Support',\n",
       " 'Servers',\n",
       " 'Production',\n",
       " 'Development',\n",
       " 'Replication',\n",
       " 'Cluster',\n",
       " 'Server']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_docs[3]['proper_nouns'][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Tech Skills\n",
    "### Create a BOW representation of phrases with Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further clean corpus, only want the 3+ length non-stopwords\n",
    "STOPWORDS = stopwords.words('english')\n",
    "model_corpus = [[word for word in doc['proper_nouns'] + doc['common_nouns'] if (word.lower() not in STOPWORDS) & (len(word) > 2)] for doc in parsed_docs]\n",
    "\n",
    "# Conjoin words that are likely to be phrases\n",
    "phrases = Phrases([x for x in model_corpus], min_count=2, threshold=1)\n",
    "model_corpus = [phrases[doc] for doc in model_corpus]\n",
    "\n",
    "# de-dup - only need each word once\n",
    "model_corpus = [\" \".join(set(doc)) for doc in model_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cases Remedy Orchestration Maintenance_Support software_development business system root_analysis Maintained Possess Senior_System sales Domain_Retail Code Achievements Query Communication_Skills Technical_Skills exposure world Setting day_day January_Present Database Management_Tool basics award Enhancement Telangana f6931801c51c63b1 production_issues applications defects cobol servicenow Hyderabad_Telangana Problem Review Decision relocate Akhil Utilities issues intervention Qualities activity Flexibility Good Maintenance Skills Electronics_Engineering Technology Leadership technologies incidents orchestration year_years Zeal awards Limited_Hyderabad Electrical meetings_client automation Assistant basis portal sharing_sessions Weekly_Status effort Effective College_Engineering matter_expert Link Responsibilities_Working Tools processing years_experience ability Senior_Systems Analytical Email mainframe data users analysis years client part Polemaina domain Strengths Knowledge activities System Providing Mainframe technology interaction suppliers Adaptability decisions approach_problem skills Support projects Team_Size Walmart deliverables meetings platform years_years time business_issues Minor enhancements Suppliers automating Yadav peer_group Service Anurag Engineer_Infosys zOS transfer_knowledge tasks Teradata Project_Objective Jntuh improvement Business responsibilities retailer performance troubleshooting Retail Working management'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>academy</th>\n",
       "      <th>accenture</th>\n",
       "      <th>access</th>\n",
       "      <th>access_control</th>\n",
       "      <th>account</th>\n",
       "      <th>accounting</th>\n",
       "      <th>accounts</th>\n",
       "      <th>achievement</th>\n",
       "      <th>...</th>\n",
       "      <th>world</th>\n",
       "      <th>writing</th>\n",
       "      <th>year</th>\n",
       "      <th>year_year</th>\n",
       "      <th>year_years</th>\n",
       "      <th>years</th>\n",
       "      <th>years_experience</th>\n",
       "      <th>years_months</th>\n",
       "      <th>years_year</th>\n",
       "      <th>years_years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111354</td>\n",
       "      <td>0.123003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.077372</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124345</td>\n",
       "      <td>0.064055</td>\n",
       "      <td>0.078749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.061267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068897</td>\n",
       "      <td>0.076105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.083236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abilities   ability  academy  accenture  access  access_control  account  \\\n",
       "0        0.0  0.000000      0.0        0.0     0.0             0.0      0.0   \n",
       "1        0.0  0.142582      0.0        0.0     0.0             0.0      0.0   \n",
       "2        0.0  0.077372      0.0        0.0     0.0             0.0      0.0   \n",
       "3        0.0  0.061267      0.0        0.0     0.0             0.0      0.0   \n",
       "4        0.0  0.000000      0.0        0.0     0.0             0.0      0.0   \n",
       "\n",
       "   accounting  accounts  achievement     ...          world  writing  \\\n",
       "0         0.0       0.0          0.0     ...       0.000000      0.0   \n",
       "1         0.0       0.0          0.0     ...       0.000000      0.0   \n",
       "2         0.0       0.0          0.0     ...       0.107315      0.0   \n",
       "3         0.0       0.0          0.0     ...       0.000000      0.0   \n",
       "4         0.0       0.0          0.0     ...       0.000000      0.0   \n",
       "\n",
       "       year  year_year  year_years     years  years_experience  years_months  \\\n",
       "0  0.000000   0.122858    0.000000  0.000000          0.000000           0.0   \n",
       "1  0.111354   0.123003    0.000000  0.000000          0.000000           0.0   \n",
       "2  0.000000   0.000000    0.124345  0.064055          0.078749           0.0   \n",
       "3  0.000000   0.000000    0.000000  0.050722          0.000000           0.0   \n",
       "4  0.068897   0.076105    0.000000  0.073035          0.000000           0.0   \n",
       "\n",
       "   years_year  years_years  \n",
       "0         0.0     0.000000  \n",
       "1         0.0     0.000000  \n",
       "2         0.0     0.073002  \n",
       "3         0.0     0.057807  \n",
       "4         0.0     0.083236  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build BOW model with limited vocab size. \n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "vectors = vectorizer.fit_transform(model_corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "\n",
    "# Quick look to check that worked\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at some subsets\n",
    "#### Martin, your examples are \"tester\", \"engineer\" and \"devops\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>model</th>\n",
       "      <th>commonality</th>\n",
       "      <th>combined_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>telangana_email</td>\n",
       "      <td>0.407842</td>\n",
       "      <td>10009.953876</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>infosys</td>\n",
       "      <td>0.411067</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>48.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>regression_testing</td>\n",
       "      <td>0.450089</td>\n",
       "      <td>23.264707</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>automation_testing</td>\n",
       "      <td>0.429469</td>\n",
       "      <td>23.334722</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>consultant_consultant</td>\n",
       "      <td>0.349563</td>\n",
       "      <td>23.547246</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>testing_testing</td>\n",
       "      <td>0.396158</td>\n",
       "      <td>21.300361</td>\n",
       "      <td>106.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>chennai_tamil</td>\n",
       "      <td>0.345196</td>\n",
       "      <td>22.525762</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>requirement_analysis</td>\n",
       "      <td>0.362518</td>\n",
       "      <td>22.034411</td>\n",
       "      <td>109.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>middleware</td>\n",
       "      <td>0.288729</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>110.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>hyderabad_telangana</td>\n",
       "      <td>0.264664</td>\n",
       "      <td>10010.233255</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>software_testing</td>\n",
       "      <td>0.442976</td>\n",
       "      <td>19.932542</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>test_cases</td>\n",
       "      <td>0.541417</td>\n",
       "      <td>18.897784</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>test_test</td>\n",
       "      <td>0.462672</td>\n",
       "      <td>18.992124</td>\n",
       "      <td>134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>roles_responsibilities</td>\n",
       "      <td>0.279976</td>\n",
       "      <td>21.759493</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>integration_testing</td>\n",
       "      <td>0.257880</td>\n",
       "      <td>22.309555</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>engineer_infosys</td>\n",
       "      <td>0.212156</td>\n",
       "      <td>10009.645997</td>\n",
       "      <td>174.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>project_title</td>\n",
       "      <td>0.399496</td>\n",
       "      <td>17.730090</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>test_engineer</td>\n",
       "      <td>0.274875</td>\n",
       "      <td>20.142059</td>\n",
       "      <td>184.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>uttar_pradesh</td>\n",
       "      <td>0.228740</td>\n",
       "      <td>22.648299</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>project_management</td>\n",
       "      <td>0.298549</td>\n",
       "      <td>19.355277</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       word     model   commonality  combined_rank\n",
       "32          telangana_email  0.407842  10009.953876           36.0\n",
       "30                  infosys  0.411067   9999.000000           48.5\n",
       "19       regression_testing  0.450089     23.264707           62.0\n",
       "24       automation_testing  0.429469     23.334722           65.0\n",
       "56    consultant_consultant  0.349563     23.547246           94.0\n",
       "36          testing_testing  0.396158     21.300361          106.0\n",
       "58            chennai_tamil  0.345196     22.525762          108.0\n",
       "50     requirement_analysis  0.362518     22.034411          109.0\n",
       "92               middleware  0.288729   9999.000000          110.5\n",
       "109     hyderabad_telangana  0.264664  10010.233255          111.0\n",
       "22         software_testing  0.442976     19.932542          114.0\n",
       "8                test_cases  0.541417     18.897784          130.0\n",
       "15                test_test  0.462672     18.992124          134.0\n",
       "95   roles_responsibilities  0.279976     21.759493          160.0\n",
       "112     integration_testing  0.257880     22.309555          168.0\n",
       "169        engineer_infosys  0.212156  10009.645997          174.0\n",
       "34            project_title  0.399496     17.730090          175.0\n",
       "97            test_engineer  0.274875     20.142059          184.0\n",
       "146           uttar_pradesh  0.228740     22.648299          193.0\n",
       "84       project_management  0.298549     19.355277          195.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset the data to a selection of interest\n",
    "selection = [i for i in range(len(parsed_docs)) if \"tester\" in parsed_docs[i]['text'].lower()]\n",
    "#selection = [i for i in range(len(parsed_docs)) if \"united kingdom\" in parsed_docs[i]['countries']]\n",
    "print(len(selection))\n",
    "\n",
    "# Sum the Model scores by word, pivot\n",
    "summed_scores = df.iloc[selection].sum(axis=0).sort_values(ascending=False)\n",
    "output = pd.DataFrame({\"word\":summed_scores.index, \"model\":summed_scores.values})\n",
    "\n",
    "# Calculate the commonality of each word (or both, for zipf's law) with Zipf's law, unrecognised gets 9999\n",
    "output['commonality'] = output['word'].apply(lambda x: sum([wordcost.get(y.lower(), 9999) for y in x.split(\"_\")]))\n",
    "\n",
    "# Rank the importance of each word for this group of documents according to TF-IDF and rarity.  Higher == Better!\n",
    "output['combined_rank'] = output['model'].rank(ascending=False) + output['commonality'].rank(ascending=False)\n",
    "output.sort_values('combined_rank').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], ['india'], [], ['india'], [], ['india'], [], [], [], [], [], [], [], [], [], [], ['australia', 'canada'], [], [], ['japan'], [], [], [], ['india'], ['india'], [], ['oman', 'india', 'singapore'], ['india'], ['india'], [], [], ['india'], [], [], [], ['canada'], [], [], ['india'], [], ['canada'], [], ['india'], [], [], ['india'], ['india'], [], ['india'], [], [], ['canada'], ['india'], ['india'], [], ['india'], [], [], [], [], [], ['india'], ['india'], [], [], ['india'], [], ['brazil'], [], [], ['australia', 'uganda', 'china', 'brazil'], [], [], ['china', 'germany'], ['india'], ['india', 'nigeria'], [], [], [], [], [], [], ['india'], ['india'], ['india'], [], ['australia', 'india', 'netherlands'], [], ['india'], ['india'], ['india'], [], ['india'], [], [], ['india'], [], [], ['canada'], [], ['india'], [], [], [], [], [], [], [], [], ['india'], [], ['india'], ['india', 'china'], [], [], [], [], ['mexico'], ['india'], [], [], ['india'], [], [], [], [], [], [], ['india'], ['india'], [], [], [], [], ['australia', 'canada'], ['india'], [], [], [], ['india'], [], [], [], [], [], [], [], [], ['india'], ['india'], ['india'], ['india'], [], [], [], [], ['india'], ['india'], [], ['mauritius', 'india'], [], ['india'], ['india', 'qatar'], [], ['india'], ['india'], ['australia', 'india'], [], [], [], [], ['india'], ['india'], ['georgia', 'india'], ['slovakia', 'france', 'india', 'spain', 'sweden', 'israel', 'germany', 'italy', 'australia'], [], [], ['india'], [], [], [], [], ['georgia'], [], [], [], ['austria', 'spain', 'germany'], [], ['india'], ['india'], ['india'], [], ['china'], ['india'], [], ['india'], [], [], [], [], [], [], ['india'], [], ['india'], [], [], ['india'], [], [], ['india'], [], [], ['india', 'canada'], [], ['australia'], []]\n"
     ]
    }
   ],
   "source": [
    "print([doc['countries'] for doc in parsed_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>model</th>\n",
       "      <th>commonality</th>\n",
       "      <th>combined_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>infosys</td>\n",
       "      <td>4.264775</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>62.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>bengaluru_karnataka</td>\n",
       "      <td>4.330366</td>\n",
       "      <td>25.280052</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>hyderabad_telangana</td>\n",
       "      <td>2.882876</td>\n",
       "      <td>10010.233255</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>btech</td>\n",
       "      <td>3.023146</td>\n",
       "      <td>9999.000000</td>\n",
       "      <td>121.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>pune_maharashtra</td>\n",
       "      <td>2.858746</td>\n",
       "      <td>24.177262</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>tamil_nadu</td>\n",
       "      <td>2.855075</td>\n",
       "      <td>21.912454</td>\n",
       "      <td>179.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>bangalore_karnataka</td>\n",
       "      <td>2.561319</td>\n",
       "      <td>23.843785</td>\n",
       "      <td>189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>chennai_tamil</td>\n",
       "      <td>2.638615</td>\n",
       "      <td>22.525762</td>\n",
       "      <td>191.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>year_year</td>\n",
       "      <td>5.233915</td>\n",
       "      <td>14.291137</td>\n",
       "      <td>202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>infosys_limited</td>\n",
       "      <td>2.222470</td>\n",
       "      <td>10008.726066</td>\n",
       "      <td>202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>communication_skills</td>\n",
       "      <td>2.731369</td>\n",
       "      <td>20.684283</td>\n",
       "      <td>207.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relocate</td>\n",
       "      <td>6.181322</td>\n",
       "      <td>13.294848</td>\n",
       "      <td>209.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>computer_science</td>\n",
       "      <td>3.571857</td>\n",
       "      <td>17.352926</td>\n",
       "      <td>215.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>engineering_college</td>\n",
       "      <td>3.122767</td>\n",
       "      <td>18.543187</td>\n",
       "      <td>219.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>karnataka_email</td>\n",
       "      <td>2.228705</td>\n",
       "      <td>22.509996</td>\n",
       "      <td>244.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>years_experience</td>\n",
       "      <td>3.150196</td>\n",
       "      <td>16.838483</td>\n",
       "      <td>249.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>email_microsoft</td>\n",
       "      <td>2.336811</td>\n",
       "      <td>20.939173</td>\n",
       "      <td>258.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>operating_systems</td>\n",
       "      <td>2.621995</td>\n",
       "      <td>18.828413</td>\n",
       "      <td>267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>years_years</td>\n",
       "      <td>3.438437</td>\n",
       "      <td>13.904148</td>\n",
       "      <td>270.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>andhra_pradesh</td>\n",
       "      <td>2.097948</td>\n",
       "      <td>23.019368</td>\n",
       "      <td>272.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     word     model   commonality  combined_rank\n",
       "44                infosys  4.264775   9999.000000           62.5\n",
       "40    bengaluru_karnataka  4.330366     25.280052           71.0\n",
       "113   hyderabad_telangana  2.882876  10010.233255          115.0\n",
       "103                 btech  3.023146   9999.000000          121.5\n",
       "115      pune_maharashtra  2.858746     24.177262          149.0\n",
       "116            tamil_nadu  2.855075     21.912454          179.0\n",
       "153   bangalore_karnataka  2.561319     23.843785          189.0\n",
       "141         chennai_tamil  2.638615     22.525762          191.0\n",
       "25              year_year  5.233915     14.291137          202.0\n",
       "196       infosys_limited  2.222470  10008.726066          202.0\n",
       "129  communication_skills  2.731369     20.684283          207.0\n",
       "4                relocate  6.181322     13.294848          209.0\n",
       "70       computer_science  3.571857     17.352926          215.0\n",
       "94    engineering_college  3.122767     18.543187          219.5\n",
       "193       karnataka_email  2.228705     22.509996          244.0\n",
       "92       years_experience  3.150196     16.838483          249.5\n",
       "183       email_microsoft  2.336811     20.939173          258.0\n",
       "144     operating_systems  2.621995     18.828413          267.0\n",
       "78            years_years  3.438437     13.904148          270.0\n",
       "226        andhra_pradesh  2.097948     23.019368          272.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset the data to a selection of interest\n",
    "selection = range(len(parsed_docs))\n",
    "#selection = [i for i in range(len(parsed_docs)) if \"united kingdom\" in parsed_docs[i]['countries']]\n",
    "print(len(selection))\n",
    "\n",
    "# Sum the Model scores by word, pivot\n",
    "summed_scores = df.iloc[selection].sum(axis=0).sort_values(ascending=False)\n",
    "output = pd.DataFrame({\"word\":summed_scores.index, \"model\":summed_scores.values})\n",
    "\n",
    "# Calculate the commonality of each word (or both, for zipf's law) with Zipf's law, unrecognised gets 9999\n",
    "output['commonality'] = output['word'].apply(lambda x: sum([wordcost.get(y.lower(), 9999) for y in x.split(\"_\")]))\n",
    "\n",
    "# Rank the importance of each word for this group of documents according to TF-IDF and rarity.  Higher == Better!\n",
    "output['combined_rank'] = output['model'].rank(ascending=False) + output['commonality'].rank(ascending=False)\n",
    "output.sort_values('combined_rank').head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract employment periods\n",
    "#### Martin, your examples are 8, 20, (30 for broken)\n",
    "\n",
    "Conclusion; use NER + regex to find dates and QA that there's a valid year present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_epochs(parsed_doc):\n",
    "    \"\"\" Take one of my parsed document dicts, split the text on date. \"\"\"\n",
    "    \n",
    "    # Get all dates (removing oddities to do with describing length of experience)\n",
    "    excluded_words = ['server', 'year']\n",
    "    dates = [date for date in parsed_doc['dates']]\n",
    "    dates = [date for date in dates if sum([word in date.lower() for word in excluded_words]) == 0]\n",
    "    \n",
    "    # Alternative;\n",
    "    dates = re.findall(r\"[1|2]{1}[0-9]{3}\", parsed_doc['text'])\n",
    "    \n",
    "    dates.reverse()\n",
    "    \n",
    "    # Iterate through, chop up text at dates oldest first\n",
    "    epoch_texts = {}\n",
    "    text = parsed_doc['text']\n",
    "    for date in dates:\n",
    "        try:\n",
    "            text, extracted = text.split(date)[0:2]\n",
    "            epoch_texts[date] = extracted\n",
    "        except:\n",
    "            pass\n",
    "    return(epoch_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'November 2017'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = parsed_docs[0]['dates'][0]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Govardhana Senior Software Engineer Bengaluru Karnataka Karnataka Email Indeed b2de315d95905b68 Total experience Years Months Cloud Lending Solutions Month Salesforce Developer Oracle Years Month Core Java Developer Languages Core Java Lang Oracle programming Sales Force Developer with Designations Promotions Willing relocate Anywhere Senior Software Engineer Cloud Lending Solutions Bangalore Karnataka January 2018 Present Present Senior Consultant Oracle Bangalore Karnataka November 2016 December 2017 Staff Consultant Oracle Bangalore Karnataka January 2014 October 2016 Associate Consultant Oracle Bangalore Karnataka November 2012 December 2013 Computer Science Engineering Adithya Institute Technology Tamil Nadu September 2008 June 2012 Less than year Data Structures years years Oracle years Algorithms years Technical Proficiency Languages Core Java Lang Data Structures Algorithms Oracle programming Sales Force with Tools RADTool Jdeveloper NetBeans Eclipse developer Developer WinSCP Putty Technologies JavaScript Webservice Operating Systems Linux Windows Version control system Git-Hub Databases Oracle Middleware logic Product Oracle Versions 10x 11x 12x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'2012': ' December 2013 Computer Science Engineering Adithya Institute Technology Tamil Nadu September 2008 June ',\n",
       " '2016': ' December 2017 Staff Consultant Oracle Bangalore Karnataka January 2014 October ',\n",
       " '2018': ' Present Present Senior Consultant Oracle Bangalore Karnataka November '}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 20\n",
    "print(parsed_docs[index]['text'])\n",
    "split_epochs(parsed_docs[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
